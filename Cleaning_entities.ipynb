{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68b1adc9-50d6-41aa-8371-75e6e429cb1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "544dbe2d-fef8-4e23-83ed-2a7e1e654631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_entities(L1):\n",
    "    L1 = [item.strip() for item in L1] # Strip Whitespace\n",
    "    # Create a set to track lowercase elements and a new list for unique items\n",
    "    seen = set()\n",
    "    L2 = []\n",
    "    for item in L1:\n",
    "        if len(item)>0 and item[0] in unwanted_characters: # Remove Unwanted Characters\n",
    "            item = item.strip(\" \"+item[0]+\" \")\n",
    "        if len(item)>0 and item[-1] in unwanted_characters:\n",
    "            item = item.strip(\" \"+item[-1]+\" \")\n",
    "        if len(item)==0: # Filter Empty Items\n",
    "            continue\n",
    "        lower_item = item.lower()\n",
    "        if lower_item not in seen: # Remove Duplicates (Case-insensitive)\n",
    "            L2.append(item)\n",
    "            seen.add(lower_item)\n",
    "\n",
    "    return L2\n",
    "\n",
    "def get_entity_dictionary(root,ID):\n",
    "    json_path = os.path.join(root,ID,f\"{ID}.json\")\n",
    "    with open(json_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "def get_entities_from_relative_path(entity_dict,relative_path):\n",
    "    # Body/Results/paragraph_6.txt, Abstract/paragraph_2.txt\n",
    "    parts = relative_path.split(\"/\")\n",
    "    if \"Body\" in parts: \n",
    "        entities = entity_dict[parts[0]][parts[1]][parts[2][:-4]]\n",
    "    else:\n",
    "        entities = entity_dict[parts[0]][parts[1][:-4]]\n",
    "    return entities\n",
    "\n",
    "def find_missing_JSON(root):\n",
    "    IDS_with_JSON, IDS_without_JSON = [],[] \n",
    "    papers = os.listdir(root)\n",
    "    # len(papers) #4123\n",
    "    for ID in papers:\n",
    "        paper_path = os.path.join(root,ID)\n",
    "        paper_files = os.listdir(paper_path)\n",
    "        if f\"{ID}.json\" not in paper_files:\n",
    "            IDS_without_JSON.append(ID)\n",
    "        else:\n",
    "            IDS_with_JSON.append(ID)\n",
    "    return IDS_with_JSON, IDS_without_JSON\n",
    "\n",
    "def get_abstract_paths(root,paper_id):\n",
    "    '''\n",
    "    Returns the paths of all abstract paragraphs\n",
    "    '''\n",
    "    abstract_path = os.path.join(root,paper_id,'Abstract')\n",
    "    relative_abstract_path = os.path.join('Abstract')\n",
    "    actual_listdir = [path_name for path_name in os.listdir(abstract_path) if \"ipynb\" not in path_name]\n",
    "    num_papers = len(actual_listdir)\n",
    "    para_paths = [os.path.join(abstract_path,f\"paragraph_{i}.txt\") for i in range(1,num_papers+1)]\n",
    "    relative_para_paths = [os.path.join(relative_abstract_path,f\"paragraph_{i}.txt\") for i in range(1,num_papers+1)]\n",
    "    # ['../OUTPUT_FOLDER/4156776/Abstract/paragraph_2.txt', '../OUTPUT_FOLDER/4156776/Abstract/paragraph_1.txt']\n",
    "    return para_paths, relative_para_paths\n",
    "\n",
    "def get_body_paths(root,paper_id):\n",
    "    '''\n",
    "    Returns the paths of all body paragraphs\n",
    "    '''\n",
    "    body_path = os.path.join(root,paper_id,'Body')\n",
    "    relative_body_path = os.path.join('Body')\n",
    "    \n",
    "    section_names = os.listdir(body_path)\n",
    "    section_paths = [os.path.join(body_path,i) for i in section_names]\n",
    "    relative_section_paths = [os.path.join(relative_body_path,i) for i in section_names]\n",
    "    \n",
    "    para_paths, relative_para_paths = [], []\n",
    "    for section, relative_section in zip(section_paths, relative_section_paths):\n",
    "        actual_listdir = [path_name for path_name in os.listdir(section) if \"ipynb\" not in path_name]\n",
    "        num_papers = len(actual_listdir)\n",
    "        subsection_para_paths = [os.path.join(section,f\"paragraph_{i}.txt\") for i in range(1,num_papers+1)]\n",
    "        relative_subsection_para_paths = [os.path.join(relative_section,f\"paragraph_{i}.txt\") for i in range(1,num_papers+1)]\n",
    "        para_paths+=subsection_para_paths\n",
    "        relative_para_paths+=relative_subsection_para_paths\n",
    "    \n",
    "    return para_paths,relative_para_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5dfcc07-90fd-4ab9-bce8-0a750a67a53a",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# unwanted_characters = list(\"+-=;;.,?|<>\")\n",
    "# original_list = [\"autism\",\"biological sex\",\"autism\",\"autism\",\"autism\",\"neurobiology\",\n",
    "#       \"autism\",\"biological base\",\"autism\",\"autism\",\"biological sex\",\n",
    "#       \"neuroanatomy\",\"autism\",\"autism\",\"autism\",\"adults\",\"spatial\",\n",
    "#       \"neuroanatomy\",\n",
    "#       \"sexually dimorphic\",\"neurobiology\",\"autism\",\"measurement\",\"+\",\n",
    "#       \"Alexithymia\",\n",
    "#       \"+\",\"= grey matter +\",\"Autism\",\"autism\", \"ACC = something\"]\n",
    "# clean_list = clean_entities(original_list)\n",
    "# clean_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4664496-a08c-405a-ba00-7dbde805b4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2229370', '6086934']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Below code takes a backup of all existing JSON files\n",
    "'''\n",
    "root = \"../OUTPUT_FOLDER\"\n",
    "unwanted_characters = list(\"+-=;;.,?|<>\")\n",
    "# JSON_FOLDER = \"ALL_JSON_files\"\n",
    "# os.makedirs(JSON_FOLDER, exist_ok=True)\n",
    "\n",
    "IDS_with_JSON, IDS_without_JSON = find_missing_JSON(root)\n",
    "IDS_without_JSON\n",
    "\n",
    "# for ID in IDS_with_JSON[:2]:\n",
    "#     entity_json = get_entity_dictionary(root,ID)\n",
    "#     #print(entity_json)\n",
    "#     with open(F\"{ID}_A.json\",'w') as f:\n",
    "#         json.dump(entity_json,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "587a60ab-4e07-4146-b38d-3d00f8a19993",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 107.32it/s]\n"
     ]
    }
   ],
   "source": [
    "for ID in tqdm(IDS_with_JSON):\n",
    "    try:\n",
    "        entity_dict = get_entity_dictionary(root,ID) # original entities dictionary\n",
    "        abstract_paths, relative_abstract_paths = get_abstract_paths(root,ID)\n",
    "        body_paths, relative_body_paths = get_body_paths(root,ID)\n",
    "        for relative_path in relative_abstract_paths:\n",
    "            parts = relative_path.split(\"/\")\n",
    "            # print(f\"\\n#####START######\\nCurrent relative path = {relative_path}\\n\\n\")\n",
    "            if \"Body\" in parts: \n",
    "                original_entities = entity_dict[parts[0]][parts[1]][parts[2][:-4]]\n",
    "                # print(f\"ORIGINAL ENTITIES = {original_entities}\\n\\n\")\n",
    "                cleaned_entities = clean_entities(original_entities)\n",
    "                # print(f\"CLEANED ENTITIES = {cleaned_entities}\\n\\n\")\n",
    "                entity_dict[parts[0]][parts[1]][parts[2][:-4]] = cleaned_entities\n",
    "            else:\n",
    "                original_entities = entity_dict[parts[0]][parts[1][:-4]]\n",
    "                # print(f\"ORIGINAL ENTITIES = {original_entities}\\n\\n\")\n",
    "                cleaned_entities = clean_entities(original_entities)\n",
    "                # print(f\"CLEANED ENTITIES = {cleaned_entities}\\n\\n*******END********\")\n",
    "                entity_dict[parts[0]][parts[1][:-4]] = cleaned_entities\n",
    "                \n",
    "        for relative_path in relative_body_paths:\n",
    "            parts = relative_path.split(\"/\")\n",
    "            # print(f\"\\n#####START######\\nCurrent relative path = {relative_path}\\n\\n\")\n",
    "            if \"Body\" in parts: \n",
    "                original_entities = entity_dict[parts[0]][parts[1]][parts[2][:-4]]\n",
    "                # print(f\"ORIGINAL ENTITIES = {original_entities}\\n\\n\")\n",
    "                cleaned_entities = clean_entities(original_entities)\n",
    "                # print(f\"CLEANED ENTITIES = {cleaned_entities}\\n\\n\")\n",
    "                entity_dict[parts[0]][parts[1]][parts[2][:-4]] = cleaned_entities\n",
    "            else:\n",
    "                original_entities = entity_dict[parts[0]][parts[1][:-4]]\n",
    "                # print(f\"ORIGINAL ENTITIES = {original_entities}\\n\\n\")\n",
    "                cleaned_entities = clean_entities(original_entities)\n",
    "                # print(f\"CLEANED ENTITIES = {cleaned_entities}\\n\\n*******END********\")\n",
    "                entity_dict[parts[0]][parts[1][:-4]] = cleaned_entities\n",
    "                \n",
    "        with open(F\"CORRECT_JSON_FILES/{ID}_B.json\",'w') as f:\n",
    "            json.dump(entity_dict, f)\n",
    "    except Exception as e:\n",
    "        with open(\"errors_while_entity_cleaning.txt\",'a') as file:\n",
    "            file.write(f\"{ID}|{e}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7c3d013-746e-436f-9c33-f2f0f144d532",
   "metadata": {},
   "outputs": [],
   "source": [
    "curr = os.listdir(\"CORRECT_JSON_FILES\")\n",
    "IDS = [k.split(\"_\")[0] for k in curr]\n",
    "set1 = set(IDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d89b1edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig = os.listdir(root)\n",
    "set2 = set(orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57e1e0e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(set(), {'2229370', '6086934'})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set1.difference(set2), set2.difference(set1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc42cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
